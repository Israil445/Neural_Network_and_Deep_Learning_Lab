{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84425f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running model: best\n",
      "\n",
      "image 1/1 /mnt/A04C91DC4C91AE12/Israil/work/DL/Object_Detection/Assignment-7/cccccc/person.jpg: 480x640 23 faces, 5.3ms\n",
      "Speed: 1.7ms preprocess, 5.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      " Prediction saved: outputs/best_face_prediction.png\n",
      "Ultralytics 8.3.160 ðŸš€ Python-3.10.18 torch-2.7.1+cu118 CUDA:0 (NVIDIA GeForce RTX 2070 SUPER, 7787MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1666.8Â±861.8 MB/s, size: 167.9 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/A04C91DC4C91AE12/Israil/work/DL/Object_Detection/Assignment-7/cccccc/wider_face_yolo/val/labels.cache... 3226 images, 4 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3226/3226 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0m/mnt/A04C91DC4C91AE12/Israil/work/DL/Object_Detection/Assignment-7/cccccc/wider_face_yolo/val/images/21_Festival_Festival_21_604.jpg: 1 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 202/202 [00:15<00:00, 13.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.842      0.579       0.66      0.363\n",
      "Speed: 0.2ms preprocess, 1.8ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val4\u001b[0m\n",
      "Validation metrics saved: outputs/best_metrics.txt\n",
      "YOLOv8 Validation Metrics for best:\n",
      "Precision: 0.8417\n",
      "Recall: 0.5787\n",
      "mAP@0.5: 0.6601\n",
      "mAP@0.5:0.95: 0.3626\n",
      "\n",
      "\n",
      " Running model: yolov8n_100e\n",
      "\n",
      "image 1/1 /mnt/A04C91DC4C91AE12/Israil/work/DL/Object_Detection/Assignment-7/cccccc/person.jpg: 480x640 23 Faces, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      " Prediction saved: outputs/yolov8n_100e_face_prediction.png\n",
      "Ultralytics 8.3.160 ðŸš€ Python-3.10.18 torch-2.7.1+cu118 CUDA:0 (NVIDIA GeForce RTX 2070 SUPER, 7787MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.1Â±0.0 ms, read: 1366.1Â±713.7 MB/s, size: 214.8 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/A04C91DC4C91AE12/Israil/work/DL/Object_Detection/Assignment-7/cccccc/wider_face_yolo/val/labels.cache... 3226 images, 4 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3226/3226 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0m/mnt/A04C91DC4C91AE12/Israil/work/DL/Object_Detection/Assignment-7/cccccc/wider_face_yolo/val/images/21_Festival_Festival_21_604.jpg: 1 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 202/202 [00:14<00:00, 13.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       3226      39696      0.828       0.56       0.63      0.336\n",
      "Speed: 0.1ms preprocess, 1.7ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val5\u001b[0m\n",
      "Validation metrics saved: outputs/yolov8n_100e_metrics.txt\n",
      "YOLOv8 Validation Metrics for yolov8n_100e:\n",
      "Precision: 0.8281\n",
      "Recall: 0.5603\n",
      "mAP@0.5: 0.6302\n",
      "mAP@0.5:0.95: 0.3355\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "models = {\n",
    "    'best': './models/best.pt',\n",
    "    'yolov8n_100e': './models/yolov8n_100e.pt'\n",
    "}\n",
    "\n",
    "# Input image for prediction demo\n",
    "test_image_path = './person.jpg'\n",
    "\n",
    "# Data.yaml path for validation (WIDER FACE)\n",
    "data_yaml = './wider_face_yolo/data.yaml'\n",
    "\n",
    "# Create outputs directory\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "for model_name, model_path in models.items():\n",
    "    print(f\"\\n Running model: {model_name}\")\n",
    "\n",
    "    # Load model\n",
    "    model = YOLO(model_path)\n",
    "\n",
    "    # Prediction on test image\n",
    "    results = model(test_image_path, save=False, conf=0.5)\n",
    "\n",
    "    # Save annotated image\n",
    "    annotated_img = results[0].plot()\n",
    "    plt.imshow(annotated_img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"{model_name} Prediction\")\n",
    "    output_img_path = f'outputs/{model_name}_face_prediction.png'\n",
    "    plt.savefig(output_img_path, bbox_inches='tight', pad_inches=0.1)\n",
    "    print(f\"\\n Prediction saved: {output_img_path}\")\n",
    "\n",
    "    # Validation on WIDER FACE val set\n",
    "    metrics = model.val(data=data_yaml)\n",
    "\n",
    "    # Extract metrics: precision, recall, mAP50, mAP50-95\n",
    "    precision, recall, map50, map50_95 = metrics.box.mean_results()\n",
    "\n",
    "    # Prepare results text\n",
    "    results_text = f\"\"\"YOLOv8 Validation Metrics for {model_name}:\n",
    "Precision: {precision:.4f}\n",
    "Recall: {recall:.4f}\n",
    "mAP@0.5: {map50:.4f}\n",
    "mAP@0.5:0.95: {map50_95:.4f}\n",
    "\"\"\"\n",
    "\n",
    "    # Save metrics\n",
    "    metrics_file = f'outputs/{model_name}_metrics.txt'\n",
    "    with open(metrics_file, 'w') as f:\n",
    "        f.write(results_text)\n",
    "\n",
    "    print(f\"Validation metrics saved: {metrics_file}\")\n",
    "    print(results_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
